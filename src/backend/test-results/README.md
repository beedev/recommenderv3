# Test Results Directory

This directory contains test execution artifacts and reports. **All files in this directory are excluded from version control.**

## Directory Structure

```
test-results/
├── coverage/              # Coverage reports
│   ├── html/             # HTML coverage reports (htmlcov/)
│   ├── coverage.xml      # XML coverage for CI/CD
│   └── .coverage         # Coverage data file
├── reports/              # Test execution reports
│   ├── junit/            # JUnit XML reports for CI/CD
│   ├── html/             # HTML test reports
│   └── json/             # JSON test results
├── logs/                 # Test execution logs
│   ├── pytest.log        # Main pytest log
│   ├── unit/             # Unit test logs
│   ├── integration/      # Integration test logs
│   └── e2e/              # E2E test logs
├── benchmarks/           # Performance benchmark results
│   ├── benchmark.json    # Benchmark data
│   └── benchmark.html    # Benchmark report
└── artifacts/            # Test artifacts (screenshots, dumps, etc.)
    ├── screenshots/      # UI screenshots (if applicable)
    ├── data/             # Test data dumps
    └── traces/           # Execution traces
```

## Purpose of Each Directory

### coverage/
**Purpose**: Code coverage analysis reports

**Generated by**:
```bash
pytest --cov=app --cov-report=html --cov-report=xml
```

**Files**:
- `html/` - Interactive HTML coverage report (view in browser)
- `coverage.xml` - XML format for CI/CD integration (SonarQube, Codecov)
- `.coverage` - SQLite database with coverage data

**Usage**:
```bash
# View HTML coverage report
open test-results/coverage/html/index.html
# Windows: start test-results/coverage/html/index.html
```

---

### reports/
**Purpose**: Test execution reports for CI/CD and analysis

**Generated by**:
```bash
# JUnit XML (for CI/CD)
pytest --junit-xml=test-results/reports/junit/test-results.xml

# HTML report (human-readable)
pytest --html=test-results/reports/html/test-report.html --self-contained-html
```

**Files**:
- `junit/*.xml` - JUnit XML format for Jenkins, Azure DevOps, GitHub Actions
- `html/*.html` - HTML reports for human review
- `json/*.json` - JSON format for custom processing

**Usage in CI/CD**:
```yaml
# Azure Pipelines example
- task: PublishTestResults@2
  inputs:
    testResultsFormat: 'JUnit'
    testResultsFiles: 'test-results/reports/junit/*.xml'

# GitHub Actions example
- uses: dorny/test-reporter@v1
  with:
    name: Test Results
    path: test-results/reports/junit/*.xml
    reporter: java-junit
```

---

### logs/
**Purpose**: Detailed test execution logs for debugging

**Generated by**:
```bash
# Enable logging in pytest.ini
# Or via command line:
pytest --log-file=test-results/logs/pytest.log --log-file-level=DEBUG
```

**Organization**:
- `pytest.log` - Main log file with all test output
- `unit/` - Unit test specific logs
- `integration/` - Integration test logs
- `e2e/` - E2E test logs

**Usage**:
```bash
# Tail logs during test run
tail -f test-results/logs/pytest.log

# Search for errors
grep -i "error\|fail" test-results/logs/pytest.log
```

---

### benchmarks/
**Purpose**: Performance benchmarking results

**Generated by**: `pytest-benchmark` plugin
```bash
pytest --benchmark-only --benchmark-json=test-results/benchmarks/benchmark.json
```

**Files**:
- `benchmark.json` - Performance metrics
- `benchmark.html` - Visual benchmark report

**Usage**:
```bash
# Run benchmarks
pytest --benchmark-only

# Compare with previous benchmarks
pytest --benchmark-compare=test-results/benchmarks/benchmark.json
```

---

### artifacts/
**Purpose**: Test execution artifacts (screenshots, data dumps, traces)

**Contents**:
- `screenshots/` - UI screenshots (for visual regression or debugging)
- `data/` - Test data dumps, session exports
- `traces/` - Execution traces, profiling data

**Usage**:
- Automatically populated by tests that generate artifacts
- Useful for debugging test failures
- Can be uploaded to CI/CD for failure analysis

---

## Cleanup

Test results can grow large over time. Clean up periodically:

```bash
# Clean all test results
rm -rf test-results/*

# Keep directory structure
find test-results -mindepth 2 -delete

# Clean only old files (older than 7 days)
find test-results -type f -mtime +7 -delete

# Clean specific category
rm -rf test-results/coverage/*
```

**Automated cleanup script** (`clean-test-results.sh`):
```bash
#!/bin/bash
# Clean test results older than N days
DAYS=${1:-7}
echo "Cleaning test results older than $DAYS days..."
find test-results -type f -mtime +$DAYS -delete
echo "Cleanup complete."
```

---

## CI/CD Integration

### Generate All Reports
```bash
pytest tests/unit tests/integration \
  --cov=app \
  --cov-report=html:test-results/coverage/html \
  --cov-report=xml:test-results/coverage/coverage.xml \
  --junit-xml=test-results/reports/junit/test-results.xml \
  --html=test-results/reports/html/test-report.html --self-contained-html \
  --log-file=test-results/logs/pytest.log \
  -v
```

### Azure Pipelines Example
```yaml
- script: |
    pytest tests/unit tests/integration \
      --cov=app \
      --cov-report=xml:test-results/coverage/coverage.xml \
      --junit-xml=test-results/reports/junit/test-results.xml \
      -v
  displayName: 'Run Tests'

- task: PublishTestResults@2
  inputs:
    testResultsFormat: 'JUnit'
    testResultsFiles: 'test-results/reports/junit/*.xml'

- task: PublishCodeCoverageResults@1
  inputs:
    codeCoverageTool: 'Cobertura'
    summaryFileLocation: 'test-results/coverage/coverage.xml'
```

### GitHub Actions Example
```yaml
- name: Run Tests
  run: |
    pytest tests/unit tests/integration \
      --cov=app \
      --cov-report=xml:test-results/coverage/coverage.xml \
      --junit-xml=test-results/reports/junit/test-results.xml \
      -v

- name: Upload Coverage
  uses: codecov/codecov-action@v3
  with:
    file: test-results/coverage/coverage.xml

- name: Upload Test Results
  uses: actions/upload-artifact@v3
  with:
    name: test-results
    path: test-results/
```

---

## Quick Reference

**View coverage report**:
```bash
pytest --cov=app --cov-report=html:test-results/coverage/html
open test-results/coverage/html/index.html
```

**Generate JUnit XML**:
```bash
pytest --junit-xml=test-results/reports/junit/test-results.xml
```

**Enable detailed logging**:
```bash
pytest --log-file=test-results/logs/pytest.log --log-file-level=DEBUG -v
```

**Run with all reports**:
```bash
pytest --cov=app \
  --cov-report=html:test-results/coverage/html \
  --junit-xml=test-results/reports/junit/test-results.xml \
  --log-file=test-results/logs/pytest.log
```

**Clean up**:
```bash
rm -rf test-results/*
# Or keep structure:
find test-results -mindepth 2 -delete
```

---

## .gitignore

This directory and all its contents are excluded from version control via `.gitignore`:

```gitignore
# Test results
test-results/
htmlcov/
.coverage
coverage.xml
.pytest_cache/
pytestdebug.log
```

---

## Notes

- **Never commit test results to git** - They are build artifacts
- **Clean up regularly** - Test results can consume significant disk space
- **Archive important results** - Use CI/CD artifact storage for historical data
- **Coverage thresholds** - Set minimum coverage requirements in CI/CD

---

## Troubleshooting

**Issue**: Coverage report not generated
```bash
# Solution: Ensure pytest-cov is installed
pip install pytest-cov

# Verify plugin is loaded
pytest --version
```

**Issue**: JUnit XML not compatible with CI/CD
```bash
# Solution: Check XML format version
pytest --junit-xml=test-results/reports/junit/results.xml --junit-prefix-class
```

**Issue**: Test logs too large
```bash
# Solution: Limit log level or split logs
pytest --log-file-level=INFO  # Instead of DEBUG
# Or separate logs by test category
pytest tests/unit --log-file=test-results/logs/unit/unit.log
```
